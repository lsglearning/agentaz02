{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiaQKgBSqgP6MyT0klqUHp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsglearning/agentaz02/blob/main/agentaz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLKfF_YmdKQ9"
      },
      "outputs": [],
      "source": [
        "azure-functions\n",
        "azure-ai-documentintelligence>=1.0.0b1\n",
        "azure-search-documents>=11.4.0\n",
        "openai>=1.0.0\n",
        "azure-identity\n",
        "python-dotenv\n",
        "\n",
        "---------------\n",
        "\n",
        "{\n",
        "  \"IsEncrypted\": false,\n",
        "  \"Values\": {\n",
        "    \"AzureWebJobsStorage\": \"YOUR_STORAGE_CONNECTION_STRING\", // O UseDevelopmentStorage=true para Azurite\n",
        "    \"FUNCTIONS_WORKER_RUNTIME\": \"python\",\n",
        "    \"AzureWebJobsFeatureFlags\": \"EnableWorkerIndexing\",\n",
        "\n",
        "    \"DOC_INTELLIGENCE_ENDPOINT\": \"YOUR_DOC_INTELLIGENCE_ENDPOINT\",\n",
        "    \"DOC_INTELLIGENCE_KEY\": \"YOUR_DOC_INTELLIGENCE_KEY\",\n",
        "\n",
        "    \"AZURE_OPENAI_ENDPOINT\": \"YOUR_AZURE_OPENAI_ENDPOINT\",\n",
        "    \"AZURE_OPENAI_KEY\": \"YOUR_AZURE_OPENAI_KEY\",\n",
        "    \"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\": \"your-embedding-deployment-name\",\n",
        "\n",
        "    \"AZURE_SEARCH_ENDPOINT\": \"YOUR_AZURE_SEARCH_ENDPOINT\",\n",
        "    \"AZURE_SEARCH_KEY\": \"YOUR_AZURE_SEARCH_ADMIN_KEY\",\n",
        "    \"AZURE_SEARCH_INDEX_NAME\": \"your-legal-index-name\"\n",
        "  }\n",
        "}\n",
        "\n",
        "----------------\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import uuid\n",
        "from typing import List, Dict\n",
        "import time # Para posible espera simple (aunque no ideal para lag de creación)\n",
        "\n",
        "import azure.functions as func\n",
        "from azure.core.exceptions import HttpResponseError # Para capturar errores de SDK\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
        "from azure.ai.documentintelligence.models import AnalyzeResult, AnalyzeDocumentRequest\n",
        "from azure.search.documents import SearchClient\n",
        "from openai import AzureOpenAI, APIError # Importar APIError para manejo específico\n",
        "\n",
        "# --- Configuración y Clientes ---\n",
        "# (Incluye manejo de errores si falta configuración)\n",
        "clients_initialized = False\n",
        "doc_intelligence_client = None\n",
        "openai_client = None\n",
        "search_client = None\n",
        "openai_embedding_deployment = None\n",
        "search_index_name = None\n",
        "\n",
        "try:\n",
        "    # Cargar configuración\n",
        "    doc_intelligence_endpoint = os.environ[\"DOC_INTELLIGENCE_ENDPOINT\"]\n",
        "    doc_intelligence_key = os.environ[\"DOC_INTELLIGENCE_KEY\"]\n",
        "    openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
        "    openai_key = os.environ[\"AZURE_OPENAI_KEY\"]\n",
        "    openai_embedding_deployment = os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"]\n",
        "    search_endpoint = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
        "    search_key = os.environ[\"AZURE_SEARCH_KEY\"]\n",
        "    search_index_name = os.environ[\"AZURE_SEARCH_INDEX_NAME\"]\n",
        "\n",
        "    # Crear Clientes\n",
        "    doc_intelligence_client = DocumentIntelligenceClient(\n",
        "        endpoint=doc_intelligence_endpoint, credential=AzureKeyCredential(doc_intelligence_key)\n",
        "    )\n",
        "    openai_client = AzureOpenAI(\n",
        "        azure_endpoint=openai_endpoint,\n",
        "        api_key=openai_key,\n",
        "        api_version=\"2024-02-01\" # O versión API adecuada\n",
        "    )\n",
        "    search_client = SearchClient(\n",
        "        endpoint=search_endpoint,\n",
        "        index_name=search_index_name,\n",
        "        credential=AzureKeyCredential(search_key)\n",
        "    )\n",
        "    logging.info(\"Azure service clients initialized successfully.\")\n",
        "    clients_initialized = True\n",
        "\n",
        "except KeyError as e:\n",
        "    logging.error(f\"CRITICAL ERROR: Missing Application Setting: {e}.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"CRITICAL ERROR: Failed to initialize Azure service clients: {e}\", exc_info=True)\n",
        "\n",
        "# --- Instancia de la App de Funciones V2 ---\n",
        "app = func.FunctionApp()\n",
        "\n",
        "# --- Lógica de Fragmentación por Párrafos ---\n",
        "def chunk_by_paragraph(di_result: AnalyzeResult, filename: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Fragmenta el texto basado en los párrafos detectados por Document Intelligence.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    if not di_result.paragraphs:\n",
        "        logging.warning(f\"No paragraphs found by Document Intelligence for {filename}.\")\n",
        "        # Opcionalmente, tratar todo el contenido como un solo chunk si es corto\n",
        "        if di_result.content and len(di_result.content.strip()) > 0:\n",
        "             logging.warning(f\"Treating entire content as one chunk for {filename}.\")\n",
        "             chunk_id_str = f\"{filename}-para-0-{uuid.uuid4()}\"\n",
        "             chunks.append({\n",
        "                 \"id\": chunk_id_str.replace(\"_\", \"-\"), # Reemplazar '_' por '-' si la clave no lo permite\n",
        "                 \"content\": di_result.content.strip(),\n",
        "                 \"source_document\": filename,\n",
        "                 \"chunk_id\": 0\n",
        "                 # Añadir más metadatos si es necesario (ej. página, rol)\n",
        "             })\n",
        "        return chunks\n",
        "\n",
        "    logging.info(f\"Found {len(di_result.paragraphs)} paragraphs. Creating chunks...\")\n",
        "    for i, paragraph in enumerate(di_result.paragraphs):\n",
        "        if paragraph.content and len(paragraph.content.strip()) > 0: # Ignorar párrafos vacíos\n",
        "            chunk_id_str = f\"{filename}-para-{i}-{uuid.uuid4()}\"\n",
        "            chunks.append({\n",
        "                \"id\": chunk_id_str.replace(\"_\", \"-\"), # Asegurar IDs válidos para la clave\n",
        "                \"content\": paragraph.content.strip(),\n",
        "                \"source_document\": filename,\n",
        "                \"chunk_id\": i\n",
        "                # Podrías añadir 'paragraph.bounding_regions[0].page_number' si lo necesitas\n",
        "            })\n",
        "        else:\n",
        "             logging.info(f\"Skipping empty paragraph {i} for {filename}.\")\n",
        "\n",
        "    logging.info(f\"Generated {len(chunks)} chunks based on paragraphs for {filename}.\")\n",
        "    return chunks\n",
        "\n",
        "# --- Función Principal del Trigger ---\n",
        "@app.blob_trigger(arg_name=\"myblob\", path=\"documentos-legales/{name}\", # Ajusta tu contenedor\n",
        "                  connection=\"AzureWebJobsStorage\") # Ajusta si usas otra conexión\n",
        "def process_document_and_index(myblob: func.InputStream):\n",
        "    \"\"\"\n",
        "    Función completa V2: Trigger -> DI -> Chunking por Párrafo -> Embedding -> AI Search Index\n",
        "    \"\"\"\n",
        "    if not clients_initialized:\n",
        "        logging.error(\"Aborting: Azure service clients not initialized.\")\n",
        "        # Podrías lanzar una excepción para que el trigger reintente si es apropiado\n",
        "        return\n",
        "\n",
        "    start_time = time.time()\n",
        "    logging.info(f\"Python V2 trigger processing blob: {myblob.name}, Size: {myblob.length} Bytes\")\n",
        "    file_name = os.path.basename(myblob.name)\n",
        "\n",
        "    # Asumimos PDF por simplicidad, añade filtro si es necesario\n",
        "    # if not file_name.lower().endswith('.pdf'): ...\n",
        "\n",
        "    try:\n",
        "        # --- 1. Extraer Texto y Párrafos con Document Intelligence ---\n",
        "        logging.info(f\"Reading blob and calling Document Intelligence for {file_name}...\")\n",
        "        blob_bytes = myblob.read()\n",
        "        poller = doc_intelligence_client.begin_analyze_document(\n",
        "            \"prebuilt-layout\", # Layout es bueno para obtener párrafos\n",
        "            AnalyzeDocumentRequest(bytes_source=blob_bytes)\n",
        "        )\n",
        "        di_result: AnalyzeResult = poller.result()\n",
        "        logging.info(\"Document Intelligence analysis completed.\")\n",
        "\n",
        "        if not di_result.content or len(di_result.content.strip()) == 0:\n",
        "            logging.warning(f\"Document Intelligence returned no content for {file_name}.\")\n",
        "            return\n",
        "\n",
        "        # --- 2. Fragmentar el Texto por Párrafos ---\n",
        "        chunks_data = chunk_by_paragraph(di_result, file_name)\n",
        "        if not chunks_data:\n",
        "            logging.warning(f\"No chunks generated for {file_name}.\")\n",
        "            return\n",
        "\n",
        "        # --- 3. Generar Vectores (Embeddings) ---\n",
        "        logging.info(f\"Generating embeddings for {len(chunks_data)} chunks...\")\n",
        "        texts_to_embed = [chunk[\"content\"] for chunk in chunks_data]\n",
        "        # OpenAI recomienda reemplazar \\n para mejor rendimiento\n",
        "        texts_to_embed = [text.replace(\"\\n\", \" \") for text in texts_to_embed]\n",
        "\n",
        "        try:\n",
        "            # La llamada a create puede manejar lotes internamente hasta cierto punto\n",
        "            embedding_result = openai_client.embeddings.create(\n",
        "                model=openai_embedding_deployment,\n",
        "                input=texts_to_embed\n",
        "            )\n",
        "            # Asociar vectores a los chunks\n",
        "            for i, chunk in enumerate(chunks_data):\n",
        "                chunk[\"content_vector\"] = embedding_result.data[i].embedding\n",
        "            logging.info(f\"Embeddings generated for {len(chunks_data)} chunks.\")\n",
        "\n",
        "        except APIError as api_err:\n",
        "             logging.error(f\"Azure OpenAI API Error during embedding: {api_err}\", exc_info=True)\n",
        "             # Considerar reintento o fallo específico\n",
        "             return\n",
        "        except Exception as emb_error:\n",
        "            logging.error(f\"Generic error during embedding: {emb_error}\", exc_info=True)\n",
        "            return # Fallar si no se pueden generar embeddings\n",
        "\n",
        "        # --- 4. Subir Documentos a Azure AI Search ---\n",
        "        logging.info(f\"Uploading {len(chunks_data)} documents to index '{search_index_name}'...\")\n",
        "        try:\n",
        "            # El SDK maneja la subida en lote\n",
        "            upload_result = search_client.upload_documents(documents=chunks_data)\n",
        "            successful_uploads = sum(1 for r in upload_result if r.succeeded)\n",
        "            logging.info(f\"Upload finished. Success for {successful_uploads} out of {len(chunks_data)} documents.\")\n",
        "            # Revisar errores individuales si es necesario\n",
        "            if successful_uploads < len(chunks_data):\n",
        "                 for item_result in upload_result:\n",
        "                      if not item_result.succeeded:\n",
        "                           logging.error(f\"  Failed to index document ID {item_result.key}: {item_result.error_message} (Status: {item_result.status_code})\")\n",
        "                 # Podrías lanzar una excepción aquí si cualquier fallo es crítico\n",
        "\n",
        "        except HttpResponseError as search_err:\n",
        "            # Capturar errores específicos de la API de Search\n",
        "            logging.error(f\"Azure AI Search API Error during upload: {search_err.status_code} - {search_err.message}\", exc_info=True)\n",
        "            if search_err.status_code == 404:\n",
        "                 logging.error(f\"INDEX NOT FOUND? Verify that the index '{search_index_name}' exists and is ready.\")\n",
        "            # Aquí podrías implementar reintentos con espera exponencial para errores transitorios (ej. 503)\n",
        "            # Pero no intentar crear el índice desde aquí.\n",
        "            raise search_err # Re-lanzar para que Azure Functions gestione el reintento si está configurado\n",
        "        except Exception as search_error:\n",
        "            logging.error(f\"Generic error uploading to Azure AI Search: {search_error}\", exc_info=True)\n",
        "            raise search_error # Re-lanzar\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Unhandled error processing {file_name}: {e}\", exc_info=True)\n",
        "        # Considera enviar a dead-letter queue o similar\n",
        "        # Re-lanzar la excepción puede ser útil para que el trigger reintente (si la causa es transitoria)\n",
        "        raise e\n",
        "    finally:\n",
        "         end_time = time.time()\n",
        "         logging.info(f\"Processing for {file_name} completed in {end_time - start_time:.2f} seconds.\")"
      ]
    }
  ]
}